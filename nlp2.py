# -*- coding: utf-8 -*-
"""NLP2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gUXVprEymYDKsA1pgV5aMnjqBvBY8z-e
"""

!pip install nlp

#importação dos pacotes necessários

import nlp
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

import seaborn as sns

"""## Utilizaremos uma base de Reviews de Streamings Onlines da Rotten Tomatoes.

### O primeiro objetivo é construir um modelo de classificação de reviews, que consiga os classificar entre negativos, positivos e neutros.
"""

#importação da base

from google.colab import files
 
 
uploaded = files.upload()

"""### A base contém três colunas:
1.   Show (nome do seriado)
2.   Rating (nota de 0 a 5)
3.   Review, o comentário do usuário

Trataremos de criar uma nova coluna que classifica o Rating em:
*   Positivo
*   Neutro
*   Negativo

Classificando o Rating em (0 - 2.5 = 'negativo', 2.5 - 3.5 = 'neutro', > 3.5 = 'positivo'.


"""

# a nossa base sera o df de treino

train_df = pd.read_csv("./audience_reviews.csv")
train_df.head()

# Criação de coluna adicional, que chamaremos de Etiqueta

def nps(row):
  if row <= 2.5:
    return 'negativo'
  if row <= 3.5:
    return 'neutro'
  if row <=5:
    return 'postivo'

train_df['etiqueta'] = train_df['Rating'].apply(lambda x: nps(x))
train_df.head()

# Vamos checar quantas classificações temos em cada etiqueta

import seaborn as sns

sns.countplot(y='etiqueta', data=train_df)

"""**Fazendo a codificação dos valores de etiqueta para cada valor na base de treino**"""

le = LabelEncoder()
Y = le.fit_transform(train_df['etiqueta'])

Y

le.classes_

"""**Separando as bases de treino e teste, com uma amostra de 20% para os testes**"""

x_train, x_test, y_train, y_test = train_test_split(train_df['Review'], Y, test_size=0.2, random_state=5)

"""**Representação TF-IDF**"""

pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer()),
    ('clf', LinearSVC())
])

# Treinando o modelo

pipeline.fit(x_train, y_train)

# Testando o modelo

pred = pipeline.predict(x_test)

# Acurácia do modelo

accuracy_score(y_test, pred)

"""### Aqui conseguimos enxergar a precisão do modelo em cada uma das classificações 

*   0: negativo, onde ele acerta 83%
*   1: neutro, onde ele acerta só 42%
*   2: positivo, onde ele acerta 87%)                                                                    
                                                                              
"""

print(classification_report(y_test, pred))

"""E aqui temos uma exemplificação do que nosso modelo realiza. A partir de um review, ele atribui uma etiqueta"""

text = 'I think the last season of Game of Thrones is terrible, i hate it'

prediction = pipeline.predict({text})
le.inverse_transform(prediction)[0]

agp = train_df.groupby('Show').count().sort_values('Rating', ascending=False)
agp.head(10)

"""## Segundo objetivo: Reconhecimento de Entidades"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
import pandas as pd
import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_sm')

nltk.download("punkt")  # tokenizador
nltk.download('averaged_perceptron_tagger') # etiquetador

#usamos panda para ler toda a tabela
df = pd.read_csv('./audience_reviews.csv')

#neste laço atrelamos a string sent apenas os dados da coluna "review" que ´o que nos interessa
sent = ' '
for x in df["Review"] :
  sent +=' ' + x

#devido a uma limitação da função nlp que pode analisar um numero finito de caracteres tivemos que cortar nossa string para que a função pudesse lê-la
sent = sent[:len(sent)//21]

pessoas = ['']
sent = nlp(sent)
for ent in sent.ents:
  if(ent.label_ == 'PERSON'):
      pessoas.append(ent.text)

from collections import Counter

contador = Counter(pessoas)



tabela = pd.DataFrame.from_dict(contador, orient='index').reset_index()


tabela



"""## Terceiro objetivo: analise de sentimentos das reviews

1 . Remover caracteres especiais
"""

def is_special(text):
  rem = ''
  for i in text:
    if i.isalnum():
      rem = rem + i
    else:
      rem = rem + ' '
  return rem

  train_df.Review = train_df.Review.apply(is_special)

"""2 . Converter tudo para minúsculo"""

def to_lower(text):
  return text.lower()

train_df.Review = train_df.Review.apply(to_lower)

"""3 . Remover StopWords"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

def rem_stopwords(text):
  stop_words = set(stopwords.words('english'))
  words = word_tokenize(text)
  return [w for w in words if w not in stop_words]

train_df.Review = train_df.Review.apply(rem_stopwords)

"""4 . Semming das palavras"""

from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

def stem_txt(text):
  ss = SnowballStemmer('english')
  return " ".join([ss.stem(w) for w in text])

train_df.Review = train_df.Review.apply(stem_txt)

train_df.Review



"""### Criando o Modelo

1. Bag of Words (BOW)
"""

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

x = np.array(train_df.iloc[:,0].values)
y = np.array(train_df.etiqueta.values)
cv = CountVectorizer(max_features = 2000)
x = cv.fit_transform(train_df.Review).toarray()

print('x.shape = ', x.shape)
print('y.shape = ', y.shape)

print(y)

"""2. Divisão Treino e Teste"""

trainx, testx, trainy, testy = train_test_split(x,y,test_size=0.2, random_state=9)

print("Train shapes: x = {}, y = {}".format(trainx.shape,trainy.shape))
print("Test shapes: x = {}, y = {}".format(testx.shape,testy.shape))

"""3. Treinando o modelo os modelos"""

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

gnb,mnb,bnb = GaussianNB(), MultinomialNB(alpha=1.0,fit_prior=True), BernoulliNB(alpha=1.0,fit_prior=True)

gnb.fit(trainx,trainy)
mnb.fit(trainx,trainy)
bnb.fit(trainx,trainy)

"""4. Predições e acurácia"""

ypg = gnb.predict(testx)
ypm = mnb.predict(testx)
ypb = bnb.predict(testx)

print("Gaussian = ",accuracy_score(testy,ypg))
print("Multinomial = ",accuracy_score(testy,ypm))
print("Bernoulli = ",accuracy_score(testy,ypb))

import pickle

pickle.dump(mnb, open('model1.pkl','wb'))

"""## Utilizando o modelo"""

text1 = '''I hate it'''

f1 = is_special(text1)
f2 = to_lower(f1)
f3 = rem_stopwords(f2)
f4 = stem_txt(f3)

bow,words = [], word_tokenize(f4)
for word in words:
  bow.append(words.count(word))

word_dict = cv.vocabulary_
pickle.dump(word_dict,open('model1.pkl','wb'))

inp = []
for i in word_dict:
  inp.append(f4.count(i[0]))
y_pred = mnb.predict(np.array(inp).reshape(1,2000))

y_pred

"""## Quarto Objetivo: Wordcloud das palavras mais comentadas dos usuários

# Importando Bibliotecas
Para conseguir uma visualização gráfica por *Workcloud*, foi necessário a importação da biblioteca NLTK:
"""

import nltk
import numpy as np
import matplotlib.pyplot as plt

nltk.download('stopwords')

"""# Dados
Como o foco é demonstrar apenas o texto com os *Reviews*, ou seja, apenas os comentários, então pegamos a coluna desejada à partir do seguinte código:
"""

print(train_df['Review'])

"""# Classificação de Texto (Stopwords)
Foi necessário **Classificar o Texto** e para melhorar a visualização, e consequentemente, a análise das palavras mais frequentes, removemos as *Stopwords* com o seguinte código:
"""

from wordcloud.wordcloud import STOPWORDS
from nltk.stem.snowball import stopwords
words = train_df['Review']
all_words = " ".join(w for w in words)
stopwords = set(STOPWORDS)

"""# WordCloud
A princípio, vamos definir uma função que irá plotar nossa wordcloud. Em seguida, vamos usar a biblioteca wordcloud para gerar nossa nuvem de palavras.
"""

def plot_wordcloud(wc):
  fig, ax = plt.subplots(figsize=(14,7))
  ax.imshow(wc,interpolation='bilinear')
  ax.set_axis_off()
  plt.imshow(wc)

wc = WordCloud(stopwords=stopwords,
               background_color='black',
               width=1600,
               height=1200).generate(all_words)

plot_wordcloud(wc)

"""# Análise Específica
Para realizar uma análise sobre um tema específico, exemplificamos com uma série, no caso "Loki". Com a visualização da *WordCloud*, percebemos que as principais palavras encontradas nos comentários são **Positivas**, ou seja, bem avaliada pelos usuários da plataforma.
"""

words = train_df[train_df['Show']=='Loki']['Review']
all_words = " ".join(w for w in words)
stopwords = set(STOPWORDS)

wc = WordCloud(stopwords=stopwords,
               background_color='black',
               width=1600,
               height=1200).generate(all_words)

plot_wordcloud(wc)